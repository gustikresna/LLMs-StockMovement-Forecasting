{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPiJ0XyVf+mrVeZuKaTbKd3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORT LIBRARIES AND LOAD DATA**"
      ],
      "metadata": {
        "id": "KiY1QE516Hmj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOQazq90qb3T"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from google.colab import files, runtime\n",
        "from transformers import AutoModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dYZCwnwa6KXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load training data\n",
        "file_path_train = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Dataset/train_data.csv'\n",
        "train_df = pd.read_csv(file_path_train)\n",
        "\n",
        "# load test data\n",
        "file_path_test = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Dataset/test_data.csv'\n",
        "test_df = pd.read_csv(file_path_test)"
      ],
      "metadata": {
        "id": "9MUAFDED6QpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPARE DATASET**"
      ],
      "metadata": {
        "id": "umrcovscmT2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to concatenate headlines, situation, and eventtype with special tokens to separate columns\n",
        "def concatenate_columns(row):\n",
        "    return f\"{row['headline']} [SEP] {row['situation']} [SEP] {row['eventtype']}\"\n",
        "\n",
        "# apply function to concatenate columns headline, situation, and eventtype\n",
        "train_df['combined_text'] = train_df.apply(concatenate_columns, axis=1)\n",
        "test_df['combined_text'] = test_df.apply(concatenate_columns, axis=1)"
      ],
      "metadata": {
        "id": "gjNn_qF5KPIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out rows with no change label due to very small number of occurences\n",
        "train_df = train_df[train_df['price_direction'] != 'no change']\n",
        "test_df = test_df[test_df['price_direction'] != 'no change']"
      ],
      "metadata": {
        "id": "2Pl9wl5LCJA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# format to datetime\n",
        "train_df['start_date'] = pd.to_datetime(train_df['start_date'])\n",
        "test_df['start_date'] = pd.to_datetime(test_df['start_date'])"
      ],
      "metadata": {
        "id": "yR9_QBYmOekk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode price direction\n",
        "label_mapping = {'positive': 1, 'negative': 0}\n",
        "train_df['label'] = train_df['price_direction'].map(label_mapping)\n",
        "test_df['label'] = test_df['price_direction'].map(label_mapping)"
      ],
      "metadata": {
        "id": "r1nIjt60CJBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DEFINE FUNCTION**"
      ],
      "metadata": {
        "id": "XSlG1QEaa2wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# define function to load the model and tokenizer\n",
        "def load_model_and_tokenizer(model_name):\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model.to(device), tokenizer\n",
        "\n",
        "# define function to generate embeddings\n",
        "def get_embeddings(texts, model, tokenizer, max_length=512):\n",
        "    embeddings_list = []\n",
        "\n",
        "    for text in texts:\n",
        "        # tokenise the text with truncation and padding to the maximum length\n",
        "        encodings = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # move tokenised inputs to the specified device (GPU or CPU)\n",
        "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # pass the tokenised inputs through the model to get outputs\n",
        "            outputs = model(**encodings)\n",
        "\n",
        "        # get embeddings from hidden states\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu()\n",
        "        embeddings_list.append(embeddings)\n",
        "\n",
        "        # clear GPU memory after processing each text\n",
        "        del encodings, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return embeddings_list\n",
        "\n",
        "# define function to max pool the embeddings for each week\n",
        "def max_pooling(embeddings_list):\n",
        "    # stack embeddings into a tensor\n",
        "    stacked_embeddings = torch.stack(embeddings_list)\n",
        "    # max pooling across the embeddings (dim=0 aggregates across the batch)\n",
        "    pooled_output, _ = stacked_embeddings.max(dim=0)\n",
        "    return pooled_output\n",
        "\n",
        "def aggregate_weekly_data(group):\n",
        "    # ensure each group has permco\n",
        "    if 'permco' not in group.columns:\n",
        "        raise KeyError(\"permco is missing from the group\")\n",
        "\n",
        "    # aggregate non-embedding columns into a single representative row for the week\n",
        "    # all non-embedding columns actually have the same value for a week\n",
        "    aggregated = {\n",
        "        'start_date': group['start_date'].iloc[0], # use the first start_date as representative of the week\n",
        "        'ticker': group['ticker'].iloc[-1], # use the most recent ticker\n",
        "        'weekly_ret': group['weekly_ret'].mean(), # get avg weekly return\n",
        "        'adj_prc': group['adj_prc'].mean(), # get avg adjusted price\n",
        "        'price_direction': group['price_direction'].iloc[0],  # use the first price direction value\n",
        "        'permco': group['permco'].iloc[0]}  # retain the permco identifier\n",
        "\n",
        "    # aggregate embeddings for each week\n",
        "    embeddings_list = list(group['embeddings'])\n",
        "    aggregated_embedding = max_pooling(embeddings_list)\n",
        "\n",
        "    # ensure the aggregated embedding is a 1D tensor\n",
        "    if aggregated_embedding.dim() > 1:\n",
        "        aggregated_embedding = aggregated_embedding.squeeze()\n",
        "\n",
        "    # validate the aggregated embedding is a 1D tensor\n",
        "    assert aggregated_embedding.ndim == 1, f\"Expected 1D tensor, got {aggregated_embedding.ndim}D tensor.\"\n",
        "\n",
        "    # add each dimension of the aggregated embedding as a separate column\n",
        "    for i, value in enumerate(aggregated_embedding.numpy()):\n",
        "        aggregated[f'embedding_{i}'] = value\n",
        "\n",
        "    return pd.Series(aggregated)\n",
        "\n",
        "# define function to generate embeddings to each row and aggregate weekly\n",
        "def add_embeddings_to_dataframe(df, model_name):\n",
        "    # load the specified model and tokeniser\n",
        "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "\n",
        "    # generate embeddings for the texts in the dataframe\n",
        "    embeddings_list = get_embeddings(df['combined_text'].tolist(), model, tokenizer)\n",
        "\n",
        "    # add embeddings to the dataframe\n",
        "    df = df.copy()\n",
        "    df['embeddings'] = embeddings_list\n",
        "\n",
        "    # aggregate the rows on a weekly basis\n",
        "    weekly_aggregated_df = df.groupby(['start_date', 'permco']).apply(aggregate_weekly_data).reset_index(drop=True)\n",
        "\n",
        "    return weekly_aggregated_df\n"
      ],
      "metadata": {
        "id": "MYc6VHKnoqXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GENERATE EMBEDDINGS**"
      ],
      "metadata": {
        "id": "77rhnTGlpv-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BERT**"
      ],
      "metadata": {
        "id": "ya1eag18-Yff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for train_df\n",
        "final_train_df = add_embeddings_to_dataframe(train_df, 'bert-base-uncased')"
      ],
      "metadata": {
        "id": "50qZx-hH-Yfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for test_df\n",
        "final_test_df = add_embeddings_to_dataframe(test_df, 'bert-base-uncased')"
      ],
      "metadata": {
        "id": "mKjdMB57-Yfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the file path\n",
        "train_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Embedding/Pre-Trained/bert_train_pretrained.csv'\n",
        "test_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Embedding/Pre-Trained/bert_test_pretrained.csv'\n",
        "\n",
        "# save to csv\n",
        "final_train_df.to_csv(train_path, index=False)\n",
        "final_test_df.to_csv(test_path, index=False)"
      ],
      "metadata": {
        "id": "XYVvyR0J-Yfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "MgseRDzw-Yfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RoBERTa**"
      ],
      "metadata": {
        "id": "fygi8dBTkk_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for train_df\n",
        "final_train_df = add_embeddings_to_dataframe(train_df, 'roberta-base')"
      ],
      "metadata": {
        "id": "hQPx_pGr01lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for test_df\n",
        "final_test_df = add_embeddings_to_dataframe(test_df, 'roberta-base')"
      ],
      "metadata": {
        "id": "WazIPm4T01lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the file path\n",
        "train_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Embedding/Pre-Trained/roberta_train_pretrained.csv'\n",
        "test_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Embedding/Pre-Trained/roberta_test_pretrained.csv'\n",
        "\n",
        "# save to csv\n",
        "final_train_df.to_csv(train_path, index=False)\n",
        "final_test_df.to_csv(test_path, index=False)"
      ],
      "metadata": {
        "id": "wJoCr5_r01ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "0BS8gOnF01lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DistilBERT**"
      ],
      "metadata": {
        "id": "9H7e2AMfopqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for train_df\n",
        "final_train_df = add_embeddings_to_dataframe(train_df, 'distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "u3lkbuui1ioN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for test_df\n",
        "final_test_df = add_embeddings_to_dataframe(test_df, 'distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "cJZoqFmo1ioQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the file path\n",
        "train_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Embedding/Pre-Trained/distilbert_train_pretrained.csv'\n",
        "test_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Embedding/Pre-Trained/distilbert_test_pretrained.csv'\n",
        "\n",
        "# save to csv\n",
        "final_train_df.to_csv(train_path, index=False)\n",
        "final_test_df.to_csv(test_path, index=False)"
      ],
      "metadata": {
        "id": "34OOXAkp1ioP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "CyXa-2D21ioR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DistilRoBERTa**"
      ],
      "metadata": {
        "id": "IYlgwkSwpjFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for train_df\n",
        "final_train_df = add_embeddings_to_dataframe(train_df, 'distilroberta-base')"
      ],
      "metadata": {
        "id": "9a1d-5-D2EPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for test_df\n",
        "final_test_df = add_embeddings_to_dataframe(test_df, 'distilroberta-base')"
      ],
      "metadata": {
        "id": "-WcoaImM2EPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the file path\n",
        "train_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Embedding/Pre-Trained/distilroberta_train_pretrained.csv'\n",
        "test_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Embedding/Pre-Trained/distilroberta_test_pretrained.csv'\n",
        "\n",
        "# save to csv\n",
        "final_train_df.to_csv(train_path, index=False)\n",
        "final_test_df.to_csv(test_path, index=False)"
      ],
      "metadata": {
        "id": "h5ZmOQcw2EPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "GnNq-ILl2EPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FinBERT**"
      ],
      "metadata": {
        "id": "kGiNipkd7vPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for train_df\n",
        "final_train_df = add_embeddings_to_dataframe(train_df, 'yiyanghkust/finbert-tone')"
      ],
      "metadata": {
        "id": "1eVyl2NL2T5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for test_df\n",
        "final_test_df = add_embeddings_to_dataframe(test_df, 'yiyanghkust/finbert-tone')"
      ],
      "metadata": {
        "id": "yk3fD_Sk2T5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the file path\n",
        "train_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Embedding/Pre-Trained/finbert_train_pretrained.csv'\n",
        "test_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Embedding/Pre-Trained/finbert_test_pretrained.csv'\n",
        "\n",
        "# save to csv\n",
        "final_train_df.to_csv(train_path, index=False)\n",
        "final_test_df.to_csv(test_path, index=False)"
      ],
      "metadata": {
        "id": "77l4vvJx2T5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "g5_QV3hF2T5T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}