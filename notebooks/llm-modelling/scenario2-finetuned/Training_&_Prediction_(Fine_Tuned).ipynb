{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plcoKQnkQns4"
      },
      "source": [
        "# **IMPORT LIBRARIES AND LOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1j9xOeYNZ12"
      },
      "outputs": [],
      "source": [
        "# install packages\n",
        "!pip install transformers[torch]\n",
        "!pip install --upgrade accelerate\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOQazq90qb3T"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.functional import softmax\n",
        "from google.colab import files, runtime\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig, Trainer, TrainingArguments\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from typing import List, Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ4i659oPGHb"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z5d6Ff94RvU"
      },
      "outputs": [],
      "source": [
        "# load training data\n",
        "file_path_train = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Dataset/train_data.csv'\n",
        "train_df = pd.read_csv(file_path_train)\n",
        "\n",
        "# load test data\n",
        "file_path_test = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Dataset/test_data.csv'\n",
        "test_df = pd.read_csv(file_path_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine train and test dataset for rollig window\n",
        "final_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
        "\n",
        "# sort by permco and date\n",
        "final_df.sort_values(by=['permco', 'start_date'], inplace=True)\n",
        "\n",
        "# format to date\n",
        "final_df['start_date'] = pd.to_datetime(final_df['start_date'], format='%Y-%m-%d')\n",
        "\n",
        "# filter out no change label due to very small occurence\n",
        "final_df = final_df[final_df['price_direction'] != 'no change']"
      ],
      "metadata": {
        "id": "-p_7-7CPPGxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IIqtmbs58B9"
      },
      "outputs": [],
      "source": [
        "# encode price direction\n",
        "label_mapping = {'positive': 1, 'negative': 0}\n",
        "final_df['label'] = final_df['price_direction'].map(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je-yu8Kc398l"
      },
      "outputs": [],
      "source": [
        "# define function to concatenate with special tokens to separate columns\n",
        "def concatenate_columns(row):\n",
        "    return f\"{row['headline']} [SEP] {row['situation']} [SEP] {row['eventtype']}\"\n",
        "\n",
        "# apply function to concatenate columns headline, situation, and eventtype\n",
        "final_df['combined_text'] = final_df.apply(concatenate_columns, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDLEC-EkhBGj"
      },
      "source": [
        "# **DEFINE FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set device to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define a custom Dataset class for processing news data\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512, max_articles=5, for_prediction=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.max_articles = max_articles\n",
        "        self.for_prediction = for_prediction\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the total number of items in the dataset\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # retrieve the item and label by index\n",
        "        item, label = self.data[idx]\n",
        "\n",
        "        # if for_prediction is True, use only the first combined text article; otherwise, use up to max_articles\n",
        "        if self.for_prediction:\n",
        "            articles = [item['combined_text']]\n",
        "        else:\n",
        "            articles = item['combined_text'][:self.max_articles]\n",
        "            if len(articles) < self.max_articles:\n",
        "                articles += [''] * (self.max_articles - len(articles))\n",
        "\n",
        "        # encode the text articles using the tokenizer\n",
        "        encoded_inputs = self.tokenizer(\n",
        "            articles,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # squeeze the first dimension if for_prediction is True, otherwise keep the full tensor\n",
        "        return {\n",
        "            'input_ids': encoded_inputs['input_ids'].squeeze(0) if self.for_prediction else encoded_inputs['input_ids'],\n",
        "            'attention_mask': encoded_inputs['attention_mask'].squeeze(0) if self.for_prediction else encoded_inputs['attention_mask'],\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "XfUFBZWCmeoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom data collator\n",
        "@dataclass\n",
        "class CustomDataCollator:\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        # stack the 'input_ids' from each feature into a single tensor for the batch\n",
        "        batch = {\n",
        "            'input_ids': torch.stack([f['input_ids'] for f in features]),\n",
        "            # stack the 'attention_mask' from each feature into a single tensor for the batch\n",
        "            'attention_mask': torch.stack([f['attention_mask'] for f in features]),\n",
        "            # convert the 'labels' from each feature into a single tensor of type long for the batch\n",
        "            'labels': torch.tensor([f['labels'] for f in features], dtype=torch.long)\n",
        "        }\n",
        "        return batch"
      ],
      "metadata": {
        "id": "6EA7nigJ5n7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a custom model class for stock prediction using a transformer model\n",
        "class TransformerForStockPrediction(nn.Module):\n",
        "    def __init__(self, transformer_model, config):\n",
        "        super().__init__()\n",
        "        # initialise the transformer model\n",
        "        self.transformer = transformer_model\n",
        "        # define a linear classifier layer that maps the transformer's output to the number of labels\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        # check if input is for prediction (2D) or training (3D)\n",
        "        if input_ids.dim() == 2:  # For prediction (single article)\n",
        "            # pass input through transformer\n",
        "            outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
        "            # get pooled output from transformer\n",
        "            if hasattr(outputs, 'pooler_output'):\n",
        "                pooled_output = outputs.pooler_output\n",
        "            else:\n",
        "                pooled_output = outputs.last_hidden_state[:, 0]\n",
        "        else:  # for training (multiple articles)\n",
        "            # reshape input for multiple articles\n",
        "            batch_size, num_articles, seq_len = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, seq_len)\n",
        "            attention_mask = attention_mask.view(-1, seq_len)\n",
        "\n",
        "            # pass reshaped input through transformer\n",
        "            outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
        "            # get pooled output from transformer\n",
        "            if hasattr(outputs, 'pooler_output'):\n",
        "                pooled_output = outputs.pooler_output\n",
        "            else:\n",
        "                pooled_output = outputs.last_hidden_state[:, 0]\n",
        "\n",
        "            # reshape and max pool across articles\n",
        "            pooled_output = pooled_output.view(batch_size, num_articles, -1)\n",
        "            pooled_output = torch.max(pooled_output, dim=1)[0]\n",
        "\n",
        "        # pass pooled output through classifier\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        # calculate loss if labels are provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        # return sequenceclassifieroutput object with relevant information\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "PUEwJb4S52co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to calculate f1 score\n",
        "def compute_f1_metrics(pred):\n",
        "    # extract true labels from the prediction object\n",
        "    labels = pred.label_ids\n",
        "    # get predicted labels by taking the argmax of the prediction probabilities\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    # compute the weighted F1 score\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    return {\"f1\": f1}"
      ],
      "metadata": {
        "id": "0NCASt6U53q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to split the dataframe into training, validation, and test\n",
        "def split_data(df, train_range, val_range, predict_range):\n",
        "    train_df = df[(df['start_date'] >= train_range[0]) & (df['start_date'] <= train_range[1])]\n",
        "    val_df = df[(df['start_date'] >= val_range[0]) & (df['start_date'] <= val_range[1])]\n",
        "    predict_df = df[(df['start_date'] >= predict_range[0]) & (df['start_date'] <= predict_range[1])]\n",
        "    return train_df, val_df, predict_df"
      ],
      "metadata": {
        "id": "7VCmVtWD55Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define dunction to group data by week for training\n",
        "def group_data(dataframe, for_prediction=False):\n",
        "    if for_prediction:\n",
        "        return [(row.to_dict(), row['label']) for _, row in dataframe.iterrows()]\n",
        "    else:\n",
        "        grouped = dataframe.groupby(['permco', 'start_date'])\n",
        "        grouped_data = []\n",
        "        for (permco, start_date), group in grouped:\n",
        "            week_data = {\n",
        "                'permco': permco,\n",
        "                'start_date': start_date,\n",
        "                'combined_text': group['combined_text'].tolist()\n",
        "            }\n",
        "            label = group['label'].iloc[0]\n",
        "            grouped_data.append((week_data, label))\n",
        "        return grouped_data"
      ],
      "metadata": {
        "id": "_gIido9A57U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to train and predict in rolling window with default learning hyperparameters\n",
        "def train_and_predict(df, model_name, batch_size=8, num_epochs=3, learning_rate=2e-5, warmup_steps=1000, weight_decay=0.01):\n",
        "    # initialise lists to store results\n",
        "    results_accuracy = []\n",
        "    results_prediction = []\n",
        "\n",
        "    # initialize tokeniser and config\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    config = AutoConfig.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    # get unique permco\n",
        "    unique_permcos = df['permco'].unique()\n",
        "\n",
        "    # define rolling windows period for training and prediction\n",
        "    windows = [\n",
        "        {'train': ('2005-01-01', '2015-12-31'), 'predict': [('2016-01-01', '2016-12-31'), ('2017-01-01', '2017-12-31')]},\n",
        "        {'train': ('2007-01-01', '2017-12-31'), 'predict': [('2018-01-01', '2018-12-31'), ('2019-01-01', '2019-12-31')]},\n",
        "        {'train': ('2009-01-01', '2019-12-31'), 'predict': [('2020-01-01', '2020-12-31'), ('2021-01-01', '2021-12-31')]},\n",
        "        {'train': ('2011-01-01', '2021-12-31'), 'predict': [('2022-01-01', '2022-12-31'), ('2023-01-01', '2023-12-31')]},\n",
        "    ]\n",
        "\n",
        "    # iterate over each permco\n",
        "    for permco in unique_permcos:\n",
        "        print(f\"Processing Permco {permco}\")\n",
        "\n",
        "        # filter dataframe for current company\n",
        "        permco_df = df[df['permco'] == permco]\n",
        "\n",
        "        # iterate over each time window\n",
        "        for i, window in enumerate(windows):\n",
        "            print(f\"Processing Window {i + 1} for Permco {permco}\")\n",
        "\n",
        "            train_start, train_end = window['train']\n",
        "\n",
        "            # split the data for training and validation\n",
        "            train_df = permco_df[(permco_df['start_date'] >= train_start) & (permco_df['start_date'] <= train_end)]\n",
        "            val_df = train_df[(train_df['start_date'] >= pd.to_datetime(train_end) - pd.DateOffset(years=2))]\n",
        "            train_df = train_df[~train_df.index.isin(val_df.index)]\n",
        "\n",
        "            # group the data by week\n",
        "            train_data = group_data(train_df)\n",
        "            val_data = group_data(val_df)\n",
        "\n",
        "            # create dataset object\n",
        "            train_dataset = NewsDataset(train_data, tokenizer, max_length=512, max_articles=5)\n",
        "            val_dataset = NewsDataset(val_data, tokenizer, max_length=512, max_articles=5)\n",
        "\n",
        "            # initialise the model for each company and window\n",
        "            transformer_model = AutoModel.from_pretrained(model_name).to(device)\n",
        "            model = TransformerForStockPrediction(transformer_model, config).to(device)\n",
        "\n",
        "            # define training arguments\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=f'./results_permco_{permco}_window_{i+1}',\n",
        "                num_train_epochs=num_epochs,\n",
        "                per_device_train_batch_size=batch_size,\n",
        "                per_device_eval_batch_size=batch_size,\n",
        "                warmup_steps=warmup_steps,\n",
        "                weight_decay=weight_decay,\n",
        "                learning_rate=learning_rate,\n",
        "                logging_dir=f'./logs_permco_{permco}_window_{i+1}',\n",
        "                logging_steps=10,\n",
        "                evaluation_strategy=\"epoch\",\n",
        "                save_strategy=\"epoch\",\n",
        "                save_total_limit=1,\n",
        "                load_best_model_at_end=True,\n",
        "                fp16=True,\n",
        "                gradient_accumulation_steps=1,\n",
        "                eval_accumulation_steps=10,\n",
        "            )\n",
        "\n",
        "            # initialise the trainer\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=val_dataset,\n",
        "                compute_metrics=compute_f1_metrics,\n",
        "                data_collator=CustomDataCollator()\n",
        "            )\n",
        "\n",
        "            # train the model\n",
        "            trainer.train()\n",
        "\n",
        "            # predict for each year in the prediction window\n",
        "            for year_start, year_end in window['predict']:\n",
        "                predict_df = permco_df[(permco_df['start_date'] >= year_start) & (permco_df['start_date'] <= year_end)]\n",
        "\n",
        "                predict_data = group_data(predict_df, for_prediction=True)\n",
        "                predict_dataset = NewsDataset(predict_data, tokenizer, max_length=512, max_articles=1, for_prediction=True)\n",
        "\n",
        "                # make predictions using the trained model\n",
        "                predictions = trainer.predict(predict_dataset)\n",
        "                # extract the raw logits from the predictions\n",
        "                logits = predictions.predictions\n",
        "                # convert logits to probabilities using softmax function\n",
        "                probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n",
        "                # get the predicted class by taking the argmax of probabilities\n",
        "                preds = np.argmax(probabilities, axis=1)\n",
        "                # extract the actual labels from the predict_data\n",
        "                actuals = np.array([item[1] for item in predict_data])\n",
        "\n",
        "                # calculate accuracy\n",
        "                accuracy = np.mean(preds == actuals)\n",
        "\n",
        "                # store accuracy results\n",
        "                results_accuracy.append({\n",
        "                    'company': permco,\n",
        "                    'train_period': f\"{train_start[:4]}-{train_end[:4]}\",\n",
        "                    'test_period': f\"{year_start[:4]}\",\n",
        "                    'test_accuracy': accuracy,\n",
        "                })\n",
        "\n",
        "                # store prediction results\n",
        "                for idx in range(len(predictions.predictions)):\n",
        "                    results_prediction.append({\n",
        "                        'company': permco,\n",
        "                        'week_date': predict_df.iloc[idx]['start_date'],\n",
        "                        'probability_neg': probabilities[idx][0],\n",
        "                        'probability_pos': probabilities[idx][1],\n",
        "                        'prediction': preds[idx],\n",
        "                        'actual': predict_df.iloc[idx]['label']\n",
        "                    })\n",
        "\n",
        "            # clear GPU memory\n",
        "            torch.cuda.empty_cache()\n",
        "            del predictions, probabilities, preds, actuals, train_dataset, val_dataset, predict_dataset\n",
        "\n",
        "    # convert results to dataframes\n",
        "    df_accuracy = pd.DataFrame(results_accuracy)\n",
        "    df_prediction = pd.DataFrame(results_prediction)\n",
        "\n",
        "    return df_accuracy, df_prediction"
      ],
      "metadata": {
        "id": "BfsC6yOo5vfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINE-TUNING & PREDICTION**"
      ],
      "metadata": {
        "id": "K_6ciAnkCLBZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MippJqeiFvP_"
      },
      "source": [
        "## **BERT**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuning and predict in rolling window\n",
        "bert_accuracy, bert_prediction = train_and_predict(final_df, 'bert-base-uncased', batch_size=16, num_epochs=3, learning_rate=5e-05, warmup_steps=500, weight_decay=0.001)"
      ],
      "metadata": {
        "id": "_xFgc0NgeMxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to dataframe\n",
        "bert_accuracy_df = pd.DataFrame(bert_accuracy)\n",
        "bert_prediction_df = pd.DataFrame(bert_prediction)"
      ],
      "metadata": {
        "id": "SAOPlFXeWtQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save results\n",
        "path_bert1 = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/bert_rolling_finetuned_accuracy.csv'\n",
        "path_bert2 = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/bert_rolling_finetuned_prediction.csv'\n",
        "# save to csv\n",
        "bert_accuracy_df.to_csv(path_bert1, index=False)\n",
        "bert_prediction_df.to_csv(path_bert2, index=False)"
      ],
      "metadata": {
        "id": "2ZtET7x3WtQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "aktVnMakWtQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cLNEyitgIBo"
      },
      "source": [
        "## **RoBERTa**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuning and predict in rolling window\n",
        "roberta_accuracy, roberta_prediction = train_and_predict(final_df, 'roberta-base', batch_size=8, num_epochs=3, learning_rate=2e-05, warmup_steps=500, weight_decay=0.0001)"
      ],
      "metadata": {
        "id": "ZiyanI1nlpJM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to dataframe\n",
        "roberta_accuracy_df = pd.DataFrame(roberta_accuracy)\n",
        "roberta_prediction_df = pd.DataFrame(roberta_prediction)"
      ],
      "metadata": {
        "id": "xgALwFof5VDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save results\n",
        "path_roberta1 = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/roberta_rolling_finetuned_accuracy.csv'\n",
        "path_roberta2 = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/roberta_rolling_finetuned_prediction.csv'\n",
        "# save to csv\n",
        "roberta_accuracy_df.to_csv(path_roberta1, index=False)\n",
        "roberta_prediction_df.to_csv(path_roberta2, index=False)"
      ],
      "metadata": {
        "id": "UFEHzxMI5VDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "OeSctKit5VDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0dkduq038-b"
      },
      "source": [
        "## **DistilBERT**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuning and predict in rolling window\n",
        "distilbert_accuracy, distilbert_prediction = train_and_predict(final_df, 'distilbert-base-uncased', batch_size=8, num_epochs=3, learning_rate=2e-05, warmup_steps=500, weight_decay=0.0001)"
      ],
      "metadata": {
        "id": "sRyBDsah5nWV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to dataframe\n",
        "distilbert_accuracy_df = pd.DataFrame(distilbert_accuracy)\n",
        "distilbert_prediction_df = pd.DataFrame(distilbert_prediction)"
      ],
      "metadata": {
        "id": "XTHItRbf5nWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save results\n",
        "path_distilbert1 = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/distilbert_rolling_finetuned_accuracy.csv'\n",
        "path_distilbert2 = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/distilbert_rolling_finetuned_prediction.csv'\n",
        "# save to csv\n",
        "distilbert_accuracy_df.to_csv(path_distilbert1, index=False)\n",
        "distilbert_prediction_df.to_csv(path_distilbert2, index=False)"
      ],
      "metadata": {
        "id": "7z-VTgnQ5nWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "42ujiwGx5nWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XipIyYW4D0_"
      },
      "source": [
        "## **DistilRoBERTa**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuning and predict in rolling window\n",
        "distilroberta_accuracy, distilroberta_prediction = train_and_predict(final_df, 'distilroberta-base', batch_size=8, num_epochs=3, learning_rate=2e-05, warmup_steps=500, weight_decay=0.0001)"
      ],
      "metadata": {
        "id": "Q0xibkI-6I0z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to dataframe\n",
        "distilroberta_accuracy_df = pd.DataFrame(distilroberta_accuracy)\n",
        "distilroberta_prediction_df = pd.DataFrame(distilroberta_prediction)"
      ],
      "metadata": {
        "id": "zBN68c4i6I02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save results\n",
        "path_distilroberta1 = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/distilroberta_rolling_finetuned_accuracy.csv'\n",
        "path_distilroberta2 = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/distilroberta_rolling_finetuned_prediction.csv'\n",
        "# save to csv\n",
        "distilroberta_accuracy_df.to_csv(path_distilroberta1, index=False)\n",
        "distilroberta_prediction_df.to_csv(path_distilroberta2, index=False)"
      ],
      "metadata": {
        "id": "osinjjuL6I04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "from google.colab import runtime"
      ],
      "metadata": {
        "id": "wo1Debvh6I05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeGTLehbBlXa"
      },
      "source": [
        "## **FinBERT**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuning and predict in rolling window\n",
        "finbert_accuracy, finbert_prediction = train_and_predict(final_df, 'yiyanghkust/finbert-tone', batch_size=16, num_epochs=3, learning_rate=5e-05, warmup_steps=1000, weight_decay=0.0001)"
      ],
      "metadata": {
        "id": "PCAbDj7E6dYK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to dataframe\n",
        "finbert_accuracy_df = pd.DataFrame(finbert_accuracy)\n",
        "finbert_prediction_df = pd.DataFrame(finbert_prediction)"
      ],
      "metadata": {
        "id": "3-Khlbwe6dYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save results\n",
        "path_finbert1 = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/finbert_rolling_finetuned_accuracy.csv'\n",
        "path_finbert2 = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/finbert_rolling_finetuned_prediction.csv'\n",
        "# save to csv\n",
        "finbert_accuracy_df.to_csv(path_finbert1, index=False)\n",
        "finbert_prediction_df.to_csv(path_finbert2, index=False)"
      ],
      "metadata": {
        "id": "M7VDG4aM6dYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "from google.colab import runtime"
      ],
      "metadata": {
        "id": "ifxoJOgx6dYO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "plcoKQnkQns4",
        "gDLEC-EkhBGj"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMG/uxUPcy4yXJO1dt1tmFt"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}