{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PRyp9VI0pVAK",
        "JycUbSJPFwQ2",
        "wVbe2WLIF78f",
        "-O9UdnxBG8EI"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO9nki53wHyI4+QAKsHrhDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustikresna/LLMs-StockMovement-Forecasting/blob/main/Learning_Hyperparameter_Optimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORT LIBRARIES**"
      ],
      "metadata": {
        "id": "WxzpgzsDpK70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install packages\n",
        "!pip install transformers[torch]\n",
        "!pip install --upgrade accelerate\n",
        "!pip install datasets\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "6TINCpcHpxEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjaJGrzZoaA3"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_metric\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.functional import softmax\n",
        "from google.colab import files, runtime\n",
        "from transformers import BertTokenizer, EvalPrediction, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
        "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel, DistilBertTokenizer, DistilBertModel, BertForSequenceClassification, Trainer, TrainingArguments, RobertaForSequenceClassification, DistilBertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOAD DATA**"
      ],
      "metadata": {
        "id": "fiSJ-V2kpnwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FnrrMuUeprP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load training data\n",
        "file_path_train = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Dataset/train_data.csv'\n",
        "train_df = pd.read_csv(file_path_train)\n",
        "\n",
        "# load test data\n",
        "file_path_test = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Dataset/test_data.csv'\n",
        "test_df = pd.read_csv(file_path_test)"
      ],
      "metadata": {
        "id": "jA0Ql4aMp-MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# format to datetime\n",
        "train_df['start_date'] = pd.to_datetime(train_df['start_date'], format='%Y-%m-%d')\n",
        "test_df['start_date'] = pd.to_datetime(test_df['start_date'], format='%Y-%m-%d')"
      ],
      "metadata": {
        "id": "fMt8oklT-p1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out no change label due to very small occurence\n",
        "train_df = train_df[train_df['price_direction'] != 'no change']\n",
        "test_df = test_df[test_df['price_direction'] != 'no change']"
      ],
      "metadata": {
        "id": "FH6MsYa9-u_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode price_direction\n",
        "label_mapping = {'positive': 1, 'negative': 0}\n",
        "train_df['label'] = train_df['price_direction'].map(label_mapping)\n",
        "test_df['label'] = test_df['price_direction'].map(label_mapping)"
      ],
      "metadata": {
        "id": "s7iHhn78qFWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to concatenate with special tokens to separate columns\n",
        "def concatenate_columns(row):\n",
        "    return f\"[HEADLINE] {row['headline']} [SEP] [SITUATION] {row['situation']} [SEP] [EVENTTYPE] {row['eventtype']}\"\n",
        "\n",
        "# apply function to concatenate columns headline, situation, and eventtype\n",
        "train_df['combined_text'] = train_df.apply(concatenate_columns, axis=1)\n",
        "test_df['combined_text'] = test_df.apply(concatenate_columns, axis=1)"
      ],
      "metadata": {
        "id": "eCSZ5wFmqGWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use only 1 company to speed up computation\n",
        "train_df = train_df[train_df['permco'] == 21394]\n",
        "test_df = test_df[test_df['permco'] == 21394]"
      ],
      "metadata": {
        "id": "cux965i9qHdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DEFINE FUNCTION**"
      ],
      "metadata": {
        "id": "uutydxb_pPmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# custom Dataset class for text data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        # convert all texts to strings and store texts, labels, and tokenizer\n",
        "        self.texts = [str(text) for text in texts]\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of samples in the dataset\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get a single item from the dataset\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        # tokenize the text\n",
        "        encodings = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # remove the batch dimension (squeeze) from encodings\n",
        "        item = {key: val.squeeze(0) for key, val in encodings.items()}\n",
        "        # add the label to the item dictionary\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# define compute_metrics function to calculate accuracy\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(axis=1)\n",
        "    accuracy = accuracy_score(p.label_ids, preds)\n",
        "    return {'accuracy': accuracy}\n",
        "\n",
        "# define the objective function for Optuna hyperparameter optimisation\n",
        "def objective(trial):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # define hyperparameters to be optimised\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1)\n",
        "    warmup_steps = trial.suggest_int(\"warmup_steps\", 500, 1000)\n",
        "\n",
        "    # define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate,\n",
        "        warmup_steps=warmup_steps,\n",
        "        weight_decay=weight_decay,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_accuracy\",\n",
        "        fp16=True,  # enable mixed precision training\n",
        "    )\n",
        "\n",
        "    # initialise trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,  # Use the test dataset for evaluation\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # train and evaluate the model\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    return -eval_result['eval_accuracy']  # Return negative accuracy for maximization"
      ],
      "metadata": {
        "id": "6M8lHOc0pJVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HYPERPARAMETER OPTIMISATION**"
      ],
      "metadata": {
        "id": "LjIpytI4HYJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BERT**"
      ],
      "metadata": {
        "id": "PRyp9VI0pVAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the features and labels\n",
        "train_texts = train_df['combined_text'].tolist()\n",
        "train_labels = train_df['label'].tolist()\n",
        "test_texts = test_df['combined_text'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "# initialise tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# create dataset objects\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "# create a study with Optuna\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# print the best trial\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Learning rate: {trial.params['learning_rate']}\")\n",
        "print(f\"  Batch size: {trial.params['batch_size']}\")\n",
        "print(f\"  Weight decay: {trial.params['weight_decay']}\")\n",
        "print(f\"  Warmup steps: {trial.params['warmup_steps']}\")"
      ],
      "metadata": {
        "id": "-V8IdyjYpalC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "s-prQAbmsKsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RoBERTa**"
      ],
      "metadata": {
        "id": "JycUbSJPFwQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the features and labels\n",
        "train_texts = train_df['combined_text'].tolist()\n",
        "train_labels = train_df['label'].tolist()\n",
        "test_texts = test_df['combined_text'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "# initialise tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "model_name = \"roberta-base\"\n",
        "\n",
        "# create dataset objects\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "# create a study with Optuna\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=15)\n",
        "\n",
        "# print the best trial\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Learning rate: {trial.params['learning_rate']}\")\n",
        "print(f\"  Batch size: {trial.params['batch_size']}\")\n",
        "print(f\"  Weight decay: {trial.params['weight_decay']}\")\n",
        "print(f\"  Warmup steps: {trial.params['warmup_steps']}\")"
      ],
      "metadata": {
        "id": "4BA6JuJLKuw3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "NTGEWSRuFwQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DistilBERT**"
      ],
      "metadata": {
        "id": "wVbe2WLIF78f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the features and labels\n",
        "train_texts = train_df['combined_text'].tolist()\n",
        "train_labels = train_df['label'].tolist()\n",
        "test_texts = test_df['combined_text'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "# initialise tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# create dataset objects\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "# create a study with Optuna\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# print the best trial\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Learning rate: {trial.params['learning_rate']}\")\n",
        "print(f\"  Batch size: {trial.params['batch_size']}\")\n",
        "print(f\"  Weight decay: {trial.params['weight_decay']}\")\n",
        "print(f\"  Warmup steps: {trial.params['warmup_steps']}\")"
      ],
      "metadata": {
        "id": "wB4bI5MKK04a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "8gQHkj-RF78n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DistilRoBERTa**"
      ],
      "metadata": {
        "id": "-O9UdnxBG8EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the features and labels\n",
        "train_texts = train_df['combined_text'].tolist()\n",
        "train_labels = train_df['label'].tolist()\n",
        "test_texts = test_df['combined_text'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "# initialise tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
        "model_name = \"distilroberta-base\"\n",
        "\n",
        "# create dataset objects\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "# create a study with Optuna\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# print the best trial\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Learning rate: {trial.params['learning_rate']}\")\n",
        "print(f\"  Batch size: {trial.params['batch_size']}\")\n",
        "print(f\"  Weight decay: {trial.params['weight_decay']}\")\n",
        "print(f\"  Warmup steps: {trial.params['warmup_steps']}\")"
      ],
      "metadata": {
        "id": "wwEz81CBK-c5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "_Dx5foJEG8EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FinBERT**"
      ],
      "metadata": {
        "id": "sfhnKhxhHF2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the features and labels\n",
        "train_texts = train_df['combined_text'].tolist()\n",
        "train_labels = train_df['label'].tolist()\n",
        "test_texts = test_df['combined_text'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "# initialise tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "model_name = \"yiyanghkust/finbert-tone\"\n",
        "\n",
        "# create dataset objects\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "# create a study with Optuna\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=16)\n",
        "\n",
        "# print the best trial\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Learning rate: {trial.params['learning_rate']}\")\n",
        "print(f\"  Batch size: {trial.params['batch_size']}\")\n",
        "print(f\"  Weight decay: {trial.params['weight_decay']}\")\n",
        "print(f\"  Warmup steps: {trial.params['warmup_steps']}\")"
      ],
      "metadata": {
        "id": "e5NpKa0WHF2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "6LvY8UTxHF2O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}