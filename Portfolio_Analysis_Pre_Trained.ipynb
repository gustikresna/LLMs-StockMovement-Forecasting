{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMgpRPY10YsufhJtzHcDvIW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustikresna/LLMs-StockMovement-Forecasting/blob/main/Portfolio_Analysis_Pre_Trained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORT LIBRARIES AND LOAD DATA**"
      ],
      "metadata": {
        "id": "plcoKQnkQns4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNVc4aQcMtJC"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files, runtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lQ4i659oPGHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load test dataset\n",
        "test_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Dataset/test_data.csv'\n",
        "test_df = pd.read_csv(test_path)\n",
        "test_df['start_date'] = pd.to_datetime(test_df['start_date'])\n",
        "\n",
        "# group by start_date of the week since the original test_df still contains multiple instances per week\n",
        "test_df_weekly = test_df.groupby(['permco', 'start_date'])['weekly_ret'].last().reset_index()"
      ],
      "metadata": {
        "id": "dIruRUlSfw-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load S&P 500 for benchmark\n",
        "spx500_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Dataset/spx500_weekly_returns.csv'\n",
        "spx500_df = pd.read_csv(spx500_path)\n",
        "spx500_df['caldt'] = pd.to_datetime(spx500_df['caldt'])"
      ],
      "metadata": {
        "id": "-NRqdqPJPC9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load market cap of selected stocks\n",
        "marketcap_path = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Dataset/market_cap.csv'\n",
        "marketcap_df = pd.read_csv(marketcap_path)\n",
        "marketcap_df['week_start_date'] = pd.to_datetime(marketcap_df['week_start_date'])"
      ],
      "metadata": {
        "id": "bTbkHxV-ADE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT**"
      ],
      "metadata": {
        "id": "xDz4MkHgEi3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Portfolio**"
      ],
      "metadata": {
        "id": "JXaPQAnWe1UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load bert's accuracy of rolling window prediction\n",
        "bert_path_accuracy = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/bert_rolling_accuracy.csv'\n",
        "bert_accuracy = pd.read_csv(bert_path_accuracy)\n",
        "\n",
        "# load bert's prediction of rolling window prediction\n",
        "bert_path_pred = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/bert_rolling_prediction.csv'\n",
        "bert_pred = pd.read_csv(bert_path_pred)\n",
        "\n",
        "#change to datetime\n",
        "bert_pred['week_date'] = pd.to_datetime(bert_pred['week_date'])"
      ],
      "metadata": {
        "id": "Gy7UMc2tBy3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get return data\n",
        "bert_ret = pd.merge(bert_pred, test_df_weekly, how='left', left_on=['company', 'week_date'], right_on=['permco', 'start_date'])\n",
        "bert_ret.drop(['permco', 'start_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "5j_9l0IDBy3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get market cap data\n",
        "bert_ret = pd.merge(bert_ret, marketcap_df, how='left', left_on=['company', 'week_date'], right_on=['permco', 'week_start_date'])\n",
        "bert_ret.drop(['permco', 'week_start_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "l-ZwdOVoBy3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get market (S&P 500) return\n",
        "bert_df = pd.merge(bert_ret, spx500_df, left_on='week_date', right_on='caldt')\n",
        "bert_df.drop('caldt', axis=1, inplace=True)\n",
        "\n",
        "# sort\n",
        "bert_df.sort_values(by=['company', 'week_date'], inplace=True)\n",
        "bert_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "DB5pDubVc8b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_log_returns_by_date = {\n",
        "    'date': [],\n",
        "    'cum_EL_return': [],\n",
        "    'cum_ES_return': [],\n",
        "    'cum_ELS_return': [],\n",
        "    'cum_VL_return': [],\n",
        "    'cum_VS_return': [],\n",
        "    'cum_VLS_return': [],\n",
        "    'cum_market_return': []\n",
        "}\n",
        "\n",
        "# group by week_date and company to get the maximum probabilities for each company in a week\n",
        "grouped_max_neg = bert_df.loc[bert_df.groupby(['week_date', 'company'])['probability_neg'].idxmax()]\n",
        "grouped_max_pos = bert_df.loc[bert_df.groupby(['week_date', 'company'])['probability_pos'].idxmax()]\n",
        "\n",
        "# group by week_date\n",
        "grouped_neg = grouped_max_neg.groupby('week_date')\n",
        "grouped_pos = grouped_max_pos.groupby('week_date')\n",
        "\n",
        "# initialise cumulative log returns\n",
        "cum_EL_return = 0\n",
        "cum_ES_return = 0\n",
        "cum_ELS_return = 0\n",
        "cum_VL_return = 0\n",
        "cum_VS_return = 0\n",
        "cum_VLS_return = 0\n",
        "cum_market_return = 0\n",
        "\n",
        "# iterate over week\n",
        "for week in bert_df['week_date'].unique():\n",
        "    group_neg = grouped_neg.get_group(week)\n",
        "    group_pos = grouped_pos.get_group(week)\n",
        "\n",
        "    # sort by probability for positive and negative predictions\n",
        "    top_positive = group_pos.sort_values(by='probability_pos', ascending=False).head(5)\n",
        "    top_negative = group_neg.sort_values(by='probability_neg', ascending=False).head(5)\n",
        "\n",
        "    # calculate log returns\n",
        "    top_positive['log_return'] = np.log1p(top_positive['weekly_ret'])\n",
        "    top_negative['log_return'] = np.log1p(top_negative['weekly_ret'])\n",
        "    bert_df.loc[bert_df['week_date'] == week, 'log_return'] = np.log1p(bert_df.loc[bert_df['week_date'] == week, 'weekly_ret'])\n",
        "\n",
        "    # Equal-Weighted Long Log Returns\n",
        "    equal_long_log_return = top_positive['log_return'].mean()\n",
        "\n",
        "    # Equal-Weighted Short Log Returns\n",
        "    equal_short_log_return = top_negative['log_return'].mean() * -1  # Negate for short\n",
        "\n",
        "    # Equal-Weighted Long-Short Log Returns\n",
        "    equal_long_short_log_return = equal_long_log_return + equal_short_log_return\n",
        "\n",
        "    # Value-Weighted Long Log Returns based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap'].sum()\n",
        "    value_long_log_return = (top_positive['log_return'] * top_positive['market_cap']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short Log Returns based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap'].sum()\n",
        "    value_short_log_return = (top_negative['log_return'] * top_negative['market_cap']).sum() / total_market_cap_negative * -1  # Negate for short\n",
        "\n",
        "    # Value-Weighted Long-Short Log Returns\n",
        "    value_long_short_log_return = value_long_log_return + value_short_log_return\n",
        "\n",
        "    # Market Log Return (equal-weighted average of all assets in the group)\n",
        "    market_log_return = bert_df.loc[bert_df['week_date'] == week, 'log_return'].mean()\n",
        "\n",
        "    # update cumulative log returns\n",
        "    cum_EL_return += equal_long_log_return\n",
        "    cum_ES_return += equal_short_log_return\n",
        "    cum_ELS_return += equal_long_short_log_return\n",
        "    cum_VL_return += value_long_log_return\n",
        "    cum_VS_return += value_short_log_return\n",
        "    cum_VLS_return += value_long_short_log_return\n",
        "    cum_market_return += market_log_return\n",
        "\n",
        "    # append results for this date\n",
        "    cumulative_log_returns_by_date['date'].append(week)\n",
        "    cumulative_log_returns_by_date['cum_EL_return'].append(cum_EL_return)\n",
        "    cumulative_log_returns_by_date['cum_ES_return'].append(cum_ES_return)\n",
        "    cumulative_log_returns_by_date['cum_ELS_return'].append(cum_ELS_return)\n",
        "    cumulative_log_returns_by_date['cum_VL_return'].append(cum_VL_return)\n",
        "    cumulative_log_returns_by_date['cum_VS_return'].append(cum_VS_return)\n",
        "    cumulative_log_returns_by_date['cum_VLS_return'].append(cum_VLS_return)\n",
        "    cumulative_log_returns_by_date['cum_market_return'].append(cum_market_return)\n",
        "\n",
        "# convert to dataframe\n",
        "cumulative_log_returns_bert = pd.DataFrame(cumulative_log_returns_by_date)"
      ],
      "metadata": {
        "id": "XtfvzDFcBy3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save results\n",
        "path_bert = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Portfolio Analysis - Pre Trained/bert_portfolio_pretrained.csv'\n",
        "\n",
        "# define path to save results\n",
        "cumulative_log_returns_bert.to_csv(path_bert, index=False)"
      ],
      "metadata": {
        "id": "lzUC1Yf6R7Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sharpe Ratio**"
      ],
      "metadata": {
        "id": "1jVkFVIelhDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate weekly log returns from cumulative log returns\n",
        "log_returns = cumulative_log_returns_bert.set_index('date').diff().dropna()\n",
        "\n",
        "# define function to calculate Sharpe Ratio\n",
        "def calculate_sharpe_ratio(return_series, risk_free_rate=0):\n",
        "    mean_return = return_series.mean()\n",
        "    std_return = return_series.std()\n",
        "    excess_return = mean_return - risk_free_rate\n",
        "    sharpe_ratio = (excess_return / std_return) * np.sqrt(52)\n",
        "    return mean_return, std_return, sharpe_ratio\n",
        "\n",
        "sharpe_ratios = []\n",
        "\n",
        "for column in log_returns.columns:\n",
        "    mean_return, std_return, sharpe_ratio = calculate_sharpe_ratio(log_returns[column])\n",
        "    sharpe_ratios.append({\n",
        "        'Portfolio': column,\n",
        "        'Mean Return': mean_return,\n",
        "        'Standard Deviation': std_return,\n",
        "        'Sharpe Ratio': sharpe_ratio\n",
        "    })\n",
        "\n",
        "# convert to dataframe\n",
        "sharpe_ratios_df = pd.DataFrame(sharpe_ratios)"
      ],
      "metadata": {
        "id": "p_ChFYAxlkCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharpe_ratios_df"
      ],
      "metadata": {
        "id": "EPV01tMelj3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "QQGAfGWbC0LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RoBERTa**"
      ],
      "metadata": {
        "id": "pcImGGmNEnK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Portfolio**"
      ],
      "metadata": {
        "id": "vk11kAmP7Dfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load roberta's accuracy of rolling window prediction\n",
        "roberta_path_accuracy = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/roberta_rolling_accuracy.csv'\n",
        "roberta_accuracy = pd.read_csv(roberta_path_accuracy)\n",
        "\n",
        "# load roberta's prediction of rolling window prediction\n",
        "roberta_path_pred = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/roberta_rolling_prediction.csv'\n",
        "roberta_pred = pd.read_csv(roberta_path_pred)\n",
        "\n",
        "# change to datetime\n",
        "roberta_pred['week_date'] = pd.to_datetime(roberta_pred['week_date'])"
      ],
      "metadata": {
        "id": "Y-_4Ylpn7Dfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get return data\n",
        "roberta_ret = pd.merge(roberta_pred, test_df_weekly, how='left', left_on=['company', 'week_date'], right_on=['permco', 'start_date'])\n",
        "roberta_ret.drop(['permco', 'start_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "PVXbjHqb7Dfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get market cap data\n",
        "roberta_ret = pd.merge(roberta_ret, marketcap_df, how='left', left_on=['company', 'week_date'], right_on=['permco', 'week_start_date'])\n",
        "roberta_ret.drop(['permco', 'week_start_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "zhVr9xdd7Dfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get market (S&P 500) return\n",
        "roberta_df = pd.merge(roberta_ret, spx500_df, left_on='week_date', right_on='caldt')\n",
        "roberta_df.drop('caldt', axis=1, inplace=True)\n",
        "\n",
        "# sort\n",
        "roberta_df.sort_values(by=['company', 'week_date'], inplace=True)\n",
        "roberta_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "GICl1_7_7Dfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_log_returns_by_date = {\n",
        "    'date': [],\n",
        "    'cum_EL_return': [],\n",
        "    'cum_ES_return': [],\n",
        "    'cum_ELS_return': [],\n",
        "    'cum_VL_return': [],\n",
        "    'cum_VS_return': [],\n",
        "    'cum_VLS_return': [],\n",
        "    'cum_market_return': []\n",
        "}\n",
        "\n",
        "# group by week_date and company to get the maximum probabilities for each company in a week\n",
        "grouped_max_neg = roberta_df.loc[roberta_df.groupby(['week_date', 'company'])['probability_neg'].idxmax()]\n",
        "grouped_max_pos = roberta_df.loc[roberta_df.groupby(['week_date', 'company'])['probability_pos'].idxmax()]\n",
        "\n",
        "# group by week_date\n",
        "grouped_neg = grouped_max_neg.groupby('week_date')\n",
        "grouped_pos = grouped_max_pos.groupby('week_date')\n",
        "\n",
        "# initialise cumulative log returns\n",
        "cum_EL_return = 0\n",
        "cum_ES_return = 0\n",
        "cum_ELS_return = 0\n",
        "cum_VL_return = 0\n",
        "cum_VS_return = 0\n",
        "cum_VLS_return = 0\n",
        "cum_market_return = 0\n",
        "\n",
        "# iterate over week\n",
        "for week in roberta_df['week_date'].unique():\n",
        "    group_neg = grouped_neg.get_group(week)\n",
        "    group_pos = grouped_pos.get_group(week)\n",
        "\n",
        "    # sort by probability for positive and negative predictions\n",
        "    top_positive = group_pos.sort_values(by='probability_pos', ascending=False).head(5)\n",
        "    top_negative = group_neg.sort_values(by='probability_neg', ascending=False).head(5)\n",
        "\n",
        "    # calculate log returns\n",
        "    top_positive['log_return'] = np.log1p(top_positive['weekly_ret'])\n",
        "    top_negative['log_return'] = np.log1p(top_negative['weekly_ret'])\n",
        "    roberta_df.loc[roberta_df['week_date'] == week, 'log_return'] = np.log1p(roberta_df.loc[roberta_df['week_date'] == week, 'weekly_ret'])\n",
        "\n",
        "    # Equal-Weighted Long Log Returns\n",
        "    equal_long_log_return = top_positive['log_return'].mean()\n",
        "\n",
        "    # Equal-Weighted Short Log Returns\n",
        "    equal_short_log_return = top_negative['log_return'].mean() * -1  # Negate for short\n",
        "\n",
        "    # Equal-Weighted Long-Short Log Returns\n",
        "    equal_long_short_log_return = equal_long_log_return + equal_short_log_return\n",
        "\n",
        "    # Value-Weighted Long Log Returns based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap'].sum()\n",
        "    value_long_log_return = (top_positive['log_return'] * top_positive['market_cap']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short Log Returns based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap'].sum()\n",
        "    value_short_log_return = (top_negative['log_return'] * top_negative['market_cap']).sum() / total_market_cap_negative * -1  # Negate for short\n",
        "\n",
        "    # Value-Weighted Long-Short Log Returns\n",
        "    value_long_short_log_return = value_long_log_return + value_short_log_return\n",
        "\n",
        "    # Market Log Return (equal-weighted average of all assets in the group)\n",
        "    market_log_return = roberta_df.loc[roberta_df['week_date'] == week, 'log_return'].mean()\n",
        "\n",
        "    # update cumulative log returns\n",
        "    cum_EL_return += equal_long_log_return\n",
        "    cum_ES_return += equal_short_log_return\n",
        "    cum_ELS_return += equal_long_short_log_return\n",
        "    cum_VL_return += value_long_log_return\n",
        "    cum_VS_return += value_short_log_return\n",
        "    cum_VLS_return += value_long_short_log_return\n",
        "    cum_market_return += market_log_return\n",
        "\n",
        "    # append results for this date\n",
        "    cumulative_log_returns_by_date['date'].append(week)\n",
        "    cumulative_log_returns_by_date['cum_EL_return'].append(cum_EL_return)\n",
        "    cumulative_log_returns_by_date['cum_ES_return'].append(cum_ES_return)\n",
        "    cumulative_log_returns_by_date['cum_ELS_return'].append(cum_ELS_return)\n",
        "    cumulative_log_returns_by_date['cum_VL_return'].append(cum_VL_return)\n",
        "    cumulative_log_returns_by_date['cum_VS_return'].append(cum_VS_return)\n",
        "    cumulative_log_returns_by_date['cum_VLS_return'].append(cum_VLS_return)\n",
        "    cumulative_log_returns_by_date['cum_market_return'].append(cum_market_return)\n",
        "\n",
        "# convert to dataframe\n",
        "cumulative_log_returns_roberta = pd.DataFrame(cumulative_log_returns_by_date)"
      ],
      "metadata": {
        "id": "1t6Vvm_g7Dfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save results\n",
        "path_roberta = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Portfolio Analysis - Pre Trained/roberta_portfolio_pretrained.csv'\n",
        "\n",
        "# save to csv\n",
        "cumulative_log_returns_roberta.to_csv(path_roberta, index=False)"
      ],
      "metadata": {
        "id": "akZ99sck7Dfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sharpe Ratio**"
      ],
      "metadata": {
        "id": "vvKkiqF07Dfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate weekly log returns from cumulative log returns\n",
        "log_returns = cumulative_log_returns_roberta.set_index('date').diff().dropna()\n",
        "\n",
        "# define function to calculate Sharpe Ratio\n",
        "def calculate_sharpe_ratio(return_series, risk_free_rate=0):\n",
        "    mean_return = return_series.mean()\n",
        "    std_return = return_series.std()\n",
        "    excess_return = mean_return - risk_free_rate\n",
        "    sharpe_ratio = (excess_return / std_return) * np.sqrt(52)\n",
        "    return mean_return, std_return, sharpe_ratio\n",
        "\n",
        "sharpe_ratios = []\n",
        "\n",
        "for column in log_returns.columns:\n",
        "    mean_return, std_return, sharpe_ratio = calculate_sharpe_ratio(log_returns[column])\n",
        "    sharpe_ratios.append({\n",
        "        'Portfolio': column,\n",
        "        'Mean Return': mean_return,\n",
        "        'Standard Deviation': std_return,\n",
        "        'Sharpe Ratio': sharpe_ratio\n",
        "    })\n",
        "\n",
        "# convert to dataframe\n",
        "sharpe_ratios_df = pd.DataFrame(sharpe_ratios)"
      ],
      "metadata": {
        "id": "xuhlMU-C7Dfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharpe_ratios_df"
      ],
      "metadata": {
        "id": "woL7cpjr7Dfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "Ebbdb0Gj7Dfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DistilBERT**"
      ],
      "metadata": {
        "id": "nCmsvTOMEnTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Portfolio**"
      ],
      "metadata": {
        "id": "iKp7mxJB7mNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load distilbert's accuracy of rolling window prediction\n",
        "distilbert_path_accuracy = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/distilbert_rolling_accuracy.csv'\n",
        "distilbert_accuracy = pd.read_csv(distilbert_path_accuracy)\n",
        "\n",
        "# load distilbert's prediction of rolling window prediction\n",
        "distilbert_path_pred = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/distilbert_rolling_prediction.csv'\n",
        "distilbert_pred = pd.read_csv(distilbert_path_pred)\n",
        "\n",
        "# change to datetime\n",
        "distilbert_pred['week_date'] = pd.to_datetime(distilbert_pred['week_date'])"
      ],
      "metadata": {
        "id": "p4cw57R_7mNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get return data\n",
        "distilbert_ret = pd.merge(distilbert_pred, test_df_weekly, how='left', left_on=['company', 'week_date'], right_on=['permco', 'start_date'])\n",
        "distilbert_ret.drop(['permco', 'start_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "8y3tk0_C7mNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get market cap data\n",
        "distilbert_ret = pd.merge(distilbert_ret, marketcap_df, how='left', left_on=['company', 'week_date'], right_on=['permco', 'week_start_date'])\n",
        "distilbert_ret.drop(['permco', 'week_start_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "GUv_BPPt7mNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get market (S&P 500) return\n",
        "distilbert_df = pd.merge(distilbert_ret, spx500_df, left_on='week_date', right_on='caldt')\n",
        "distilbert_df.drop('caldt', axis=1, inplace=True)\n",
        "\n",
        "# sort\n",
        "distilbert_df.sort_values(by=['company', 'week_date'], inplace=True)\n",
        "distilbert_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "nU7-l97v7mNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_log_returns_by_date = {\n",
        "    'date': [],\n",
        "    'cum_EL_return': [],\n",
        "    'cum_ES_return': [],\n",
        "    'cum_ELS_return': [],\n",
        "    'cum_VL_return': [],\n",
        "    'cum_VS_return': [],\n",
        "    'cum_VLS_return': [],\n",
        "    'cum_market_return': []\n",
        "}\n",
        "\n",
        "# group by week_date and company to get the maximum probabilities for each company in a week\n",
        "grouped_max_neg = distilbert_df.loc[distilbert_df.groupby(['week_date', 'company'])['probability_neg'].idxmax()]\n",
        "grouped_max_pos = distilbert_df.loc[distilbert_df.groupby(['week_date', 'company'])['probability_pos'].idxmax()]\n",
        "\n",
        "# group by week_date\n",
        "grouped_neg = grouped_max_neg.groupby('week_date')\n",
        "grouped_pos = grouped_max_pos.groupby('week_date')\n",
        "\n",
        "# initialise cumulative log returns\n",
        "cum_EL_return = 0\n",
        "cum_ES_return = 0\n",
        "cum_ELS_return = 0\n",
        "cum_VL_return = 0\n",
        "cum_VS_return = 0\n",
        "cum_VLS_return = 0\n",
        "cum_market_return = 0\n",
        "\n",
        "# iterate over week\n",
        "for week in distilbert_df['week_date'].unique():\n",
        "    group_neg = grouped_neg.get_group(week)\n",
        "    group_pos = grouped_pos.get_group(week)\n",
        "\n",
        "    # sort by probability for positive and negative predictions\n",
        "    top_positive = group_pos.sort_values(by='probability_pos', ascending=False).head(5)\n",
        "    top_negative = group_neg.sort_values(by='probability_neg', ascending=False).head(5)\n",
        "\n",
        "    # calculate log returns\n",
        "    top_positive['log_return'] = np.log1p(top_positive['weekly_ret'])\n",
        "    top_negative['log_return'] = np.log1p(top_negative['weekly_ret'])\n",
        "    distilbert_df.loc[distilbert_df['week_date'] == week, 'log_return'] = np.log1p(distilbert_df.loc[distilbert_df['week_date'] == week, 'weekly_ret'])\n",
        "\n",
        "    # Equal-Weighted Long Log Returns\n",
        "    equal_long_log_return = top_positive['log_return'].mean()\n",
        "\n",
        "    # Equal-Weighted Short Log Returns\n",
        "    equal_short_log_return = top_negative['log_return'].mean() * -1  # Negate for short\n",
        "\n",
        "    # Equal-Weighted Long-Short Log Returns\n",
        "    equal_long_short_log_return = equal_long_log_return + equal_short_log_return\n",
        "\n",
        "    # Value-Weighted Long Log Returns based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap'].sum()\n",
        "    value_long_log_return = (top_positive['log_return'] * top_positive['market_cap']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short Log Returns based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap'].sum()\n",
        "    value_short_log_return = (top_negative['log_return'] * top_negative['market_cap']).sum() / total_market_cap_negative * -1  # Negate for short\n",
        "\n",
        "    # Value-Weighted Long-Short Log Returns\n",
        "    value_long_short_log_return = value_long_log_return + value_short_log_return\n",
        "\n",
        "    # Market Log Return (equal-weighted average of all assets in the group)\n",
        "    market_log_return = distilbert_df.loc[distilbert_df['week_date'] == week, 'log_return'].mean()\n",
        "\n",
        "    # update cumulative log returns\n",
        "    cum_EL_return += equal_long_log_return\n",
        "    cum_ES_return += equal_short_log_return\n",
        "    cum_ELS_return += equal_long_short_log_return\n",
        "    cum_VL_return += value_long_log_return\n",
        "    cum_VS_return += value_short_log_return\n",
        "    cum_VLS_return += value_long_short_log_return\n",
        "    cum_market_return += market_log_return\n",
        "\n",
        "    # append results for this date\n",
        "    cumulative_log_returns_by_date['date'].append(week)\n",
        "    cumulative_log_returns_by_date['cum_EL_return'].append(cum_EL_return)\n",
        "    cumulative_log_returns_by_date['cum_ES_return'].append(cum_ES_return)\n",
        "    cumulative_log_returns_by_date['cum_ELS_return'].append(cum_ELS_return)\n",
        "    cumulative_log_returns_by_date['cum_VL_return'].append(cum_VL_return)\n",
        "    cumulative_log_returns_by_date['cum_VS_return'].append(cum_VS_return)\n",
        "    cumulative_log_returns_by_date['cum_VLS_return'].append(cum_VLS_return)\n",
        "    cumulative_log_returns_by_date['cum_market_return'].append(cum_market_return)\n",
        "\n",
        "# convert to dataframe\n",
        "cumulative_log_returns_distilbert = pd.DataFrame(cumulative_log_returns_by_date)"
      ],
      "metadata": {
        "id": "PFG-ckRA7mNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save results\n",
        "path_distilbert = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Portfolio Analysis - Pre Trained/distilbert_portfolio_pretrained.csv'\n",
        "\n",
        "# save to csv\n",
        "cumulative_log_returns_distilbert.to_csv(path_distilbert, index=False)"
      ],
      "metadata": {
        "id": "KTsttxmT7mNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sharpe Ratio**"
      ],
      "metadata": {
        "id": "y68MSjqz7mNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate weekly log returns from cumulative log returns\n",
        "log_returns = cumulative_log_returns_distilbert.set_index('date').diff().dropna()\n",
        "\n",
        "# define function to calculate Sharpe Ratio\n",
        "def calculate_sharpe_ratio(return_series, risk_free_rate=0):\n",
        "    mean_return = return_series.mean()\n",
        "    std_return = return_series.std()\n",
        "    excess_return = mean_return - risk_free_rate\n",
        "    sharpe_ratio = (excess_return / std_return) * np.sqrt(52)\n",
        "    return mean_return, std_return, sharpe_ratio\n",
        "\n",
        "sharpe_ratios = []\n",
        "\n",
        "for column in log_returns.columns:\n",
        "    mean_return, std_return, sharpe_ratio = calculate_sharpe_ratio(log_returns[column])\n",
        "    sharpe_ratios.append({\n",
        "        'Portfolio': column,\n",
        "        'Mean Return': mean_return,\n",
        "        'Standard Deviation': std_return,\n",
        "        'Sharpe Ratio': sharpe_ratio\n",
        "    })\n",
        "\n",
        "# convert to dataframe\n",
        "sharpe_ratios_df = pd.DataFrame(sharpe_ratios)"
      ],
      "metadata": {
        "id": "YZiRfXwq7mNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharpe_ratios_df"
      ],
      "metadata": {
        "id": "llG9OKpg7mNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "Qea2SFhk7mNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DistilRoBERTa**"
      ],
      "metadata": {
        "id": "WbHmIYXREnd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Portfolio**"
      ],
      "metadata": {
        "id": "8oigbr8U8IAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load distilroberta's accuracy of rolling window prediction\n",
        "distilroberta_path_accuracy = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/distilroberta_rolling_accuracy.csv'\n",
        "distilroberta_accuracy = pd.read_csv(distilroberta_path_accuracy)\n",
        "\n",
        "# load distilroberta's prediction of rolling window prediction\n",
        "distilroberta_path_pred = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/distilroberta_rolling_prediction.csv'\n",
        "distilroberta_pred = pd.read_csv(distilroberta_path_pred)\n",
        "\n",
        "#change to datetime\n",
        "distilroberta_pred['week_date'] = pd.to_datetime(distilroberta_pred['week_date'])"
      ],
      "metadata": {
        "id": "kwpWjyEq8IAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get return data\n",
        "distilroberta_ret = pd.merge(distilroberta_pred, test_df_weekly, how='left', left_on=['company', 'week_date'], right_on=['permco', 'start_date'])\n",
        "distilroberta_ret.drop(['permco', 'start_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Yq74PIBB8IAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get market cap data\n",
        "distilroberta_ret = pd.merge(distilroberta_ret, marketcap_df, how='left', left_on=['company', 'week_date'], right_on=['permco', 'week_start_date'])\n",
        "distilroberta_ret.drop(['permco', 'week_start_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "EXi7otqH8IAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get market (S&P 500) return\n",
        "distilroberta_df = pd.merge(distilroberta_ret, spx500_df, left_on='week_date', right_on='caldt')\n",
        "distilroberta_df.drop('caldt', axis=1, inplace=True)\n",
        "\n",
        "# sort\n",
        "distilroberta_df.sort_values(by=['company', 'week_date'], inplace=True)\n",
        "distilroberta_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "ifWx8KU28IAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_log_returns_by_date = {\n",
        "    'date': [],\n",
        "    'cum_EL_return': [],\n",
        "    'cum_ES_return': [],\n",
        "    'cum_ELS_return': [],\n",
        "    'cum_VL_return': [],\n",
        "    'cum_VS_return': [],\n",
        "    'cum_VLS_return': [],\n",
        "    'cum_market_return': []\n",
        "}\n",
        "\n",
        "# group by week_date and company to get the maximum probabilities for each company in a week\n",
        "grouped_max_neg = distilroberta_df.loc[distilroberta_df.groupby(['week_date', 'company'])['probability_neg'].idxmax()]\n",
        "grouped_max_pos = distilroberta_df.loc[distilroberta_df.groupby(['week_date', 'company'])['probability_pos'].idxmax()]\n",
        "\n",
        "# group by week_date\n",
        "grouped_neg = grouped_max_neg.groupby('week_date')\n",
        "grouped_pos = grouped_max_pos.groupby('week_date')\n",
        "\n",
        "# initialise cumulative log returns\n",
        "cum_EL_return = 0\n",
        "cum_ES_return = 0\n",
        "cum_ELS_return = 0\n",
        "cum_VL_return = 0\n",
        "cum_VS_return = 0\n",
        "cum_VLS_return = 0\n",
        "cum_market_return = 0\n",
        "\n",
        "# iterate over week\n",
        "for week in distilroberta_df['week_date'].unique():\n",
        "    group_neg = grouped_neg.get_group(week)\n",
        "    group_pos = grouped_pos.get_group(week)\n",
        "\n",
        "    # sort by probability for positive and negative predictions\n",
        "    top_positive = group_pos.sort_values(by='probability_pos', ascending=False).head(5)\n",
        "    top_negative = group_neg.sort_values(by='probability_neg', ascending=False).head(5)\n",
        "\n",
        "    # calculate log returns\n",
        "    top_positive['log_return'] = np.log1p(top_positive['weekly_ret'])\n",
        "    top_negative['log_return'] = np.log1p(top_negative['weekly_ret'])\n",
        "    distilroberta_df.loc[distilroberta_df['week_date'] == week, 'log_return'] = np.log1p(distilroberta_df.loc[distilroberta_df['week_date'] == week, 'weekly_ret'])\n",
        "\n",
        "    # Equal-Weighted Long Log Returns\n",
        "    equal_long_log_return = top_positive['log_return'].mean()\n",
        "\n",
        "    # Equal-Weighted Short Log Returns\n",
        "    equal_short_log_return = top_negative['log_return'].mean() * -1  # Negate for short\n",
        "\n",
        "    # Equal-Weighted Long-Short Log Returns\n",
        "    equal_long_short_log_return = equal_long_log_return + equal_short_log_return\n",
        "\n",
        "    # Value-Weighted Long Log Returns based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap'].sum()\n",
        "    value_long_log_return = (top_positive['log_return'] * top_positive['market_cap']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short Log Returns based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap'].sum()\n",
        "    value_short_log_return = (top_negative['log_return'] * top_negative['market_cap']).sum() / total_market_cap_negative * -1  # Negate for short\n",
        "\n",
        "    # Value-Weighted Long-Short Log Returns\n",
        "    value_long_short_log_return = value_long_log_return + value_short_log_return\n",
        "\n",
        "    # Market Log Return (equal-weighted average of all assets in the group)\n",
        "    market_log_return = distilroberta_df.loc[distilroberta_df['week_date'] == week, 'log_return'].mean()\n",
        "\n",
        "    # update cumulative log returns\n",
        "    cum_EL_return += equal_long_log_return\n",
        "    cum_ES_return += equal_short_log_return\n",
        "    cum_ELS_return += equal_long_short_log_return\n",
        "    cum_VL_return += value_long_log_return\n",
        "    cum_VS_return += value_short_log_return\n",
        "    cum_VLS_return += value_long_short_log_return\n",
        "    cum_market_return += market_log_return\n",
        "\n",
        "    # append results for this date\n",
        "    cumulative_log_returns_by_date['date'].append(week)\n",
        "    cumulative_log_returns_by_date['cum_EL_return'].append(cum_EL_return)\n",
        "    cumulative_log_returns_by_date['cum_ES_return'].append(cum_ES_return)\n",
        "    cumulative_log_returns_by_date['cum_ELS_return'].append(cum_ELS_return)\n",
        "    cumulative_log_returns_by_date['cum_VL_return'].append(cum_VL_return)\n",
        "    cumulative_log_returns_by_date['cum_VS_return'].append(cum_VS_return)\n",
        "    cumulative_log_returns_by_date['cum_VLS_return'].append(cum_VLS_return)\n",
        "    cumulative_log_returns_by_date['cum_market_return'].append(cum_market_return)\n",
        "\n",
        "# convert to dataframe\n",
        "cumulative_log_returns_distilroberta = pd.DataFrame(cumulative_log_returns_by_date)"
      ],
      "metadata": {
        "id": "YeMOf3BI8IAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save results\n",
        "path_distilroberta = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Portfolio Analysis - Pre Trained/distilroberta_portfolio_pretrained.csv'\n",
        "\n",
        "# save to csv\n",
        "cumulative_log_returns_distilroberta.to_csv(path_distilroberta, index=False)"
      ],
      "metadata": {
        "id": "-lw9i8iM8IAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sharpe Ratio**"
      ],
      "metadata": {
        "id": "FpLhreaA8IAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate weekly log returns from cumulative log returns\n",
        "log_returns = cumulative_log_returns_distilroberta.set_index('date').diff().dropna()\n",
        "\n",
        "# define function to calculate Sharpe Ratio\n",
        "def calculate_sharpe_ratio(return_series, risk_free_rate=0):\n",
        "    mean_return = return_series.mean()\n",
        "    std_return = return_series.std()\n",
        "    excess_return = mean_return - risk_free_rate\n",
        "    sharpe_ratio = (excess_return / std_return) * np.sqrt(52)\n",
        "    return mean_return, std_return, sharpe_ratio\n",
        "\n",
        "sharpe_ratios = []\n",
        "\n",
        "for column in log_returns.columns:\n",
        "    mean_return, std_return, sharpe_ratio = calculate_sharpe_ratio(log_returns[column])\n",
        "    sharpe_ratios.append({\n",
        "        'Portfolio': column,\n",
        "        'Mean Return': mean_return,\n",
        "        'Standard Deviation': std_return,\n",
        "        'Sharpe Ratio': sharpe_ratio\n",
        "    })\n",
        "\n",
        "# convert to dataframe\n",
        "sharpe_ratios_df = pd.DataFrame(sharpe_ratios)"
      ],
      "metadata": {
        "id": "zT5Ghe2n8IAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharpe_ratios_df"
      ],
      "metadata": {
        "id": "jEml4l0r8IAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "rQ9QXV2U8IAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FinBERT**"
      ],
      "metadata": {
        "id": "JoXwg58jET9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Portfolio**"
      ],
      "metadata": {
        "id": "bYJrQGXq8mYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load finbert's accuracy of rolling window prediction\n",
        "finbert_path_accuracy = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/finbert_rolling_accuracy.csv'\n",
        "finbert_accuracy = pd.read_csv(finbert_path_accuracy)\n",
        "\n",
        "# load finbert's prediction of rolling window prediction\n",
        "finbert_path_pred = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Rolling Window Prediction/finbert_rolling_prediction.csv'\n",
        "finbert_pred = pd.read_csv(finbert_path_pred)\n",
        "\n",
        "#change to datetime\n",
        "finbert_pred['week_date'] = pd.to_datetime(finbert_pred['week_date'])"
      ],
      "metadata": {
        "id": "DP7aVQes8mYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get return data\n",
        "finbert_ret = pd.merge(finbert_pred, test_df_weekly, how='left', left_on=['company', 'week_date'], right_on=['permco', 'start_date'])\n",
        "finbert_ret.drop(['permco', 'start_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "-mEmoi9E8mYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get market cap data\n",
        "finbert_ret = pd.merge(finbert_ret, marketcap_df, how='left', left_on=['company', 'week_date'], right_on=['permco', 'week_start_date'])\n",
        "finbert_ret.drop(['permco', 'week_start_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "dVSYj5Fx8mY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge to get market (S&P 500) return\n",
        "finbert_df = pd.merge(finbert_ret, spx500_df, left_on='week_date', right_on='caldt')\n",
        "finbert_df.drop('caldt', axis=1, inplace=True)\n",
        "\n",
        "# sort\n",
        "finbert_df.sort_values(by=['company', 'week_date'], inplace=True)\n",
        "finbert_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "CbBcsiId8mY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_log_returns_by_date = {\n",
        "    'date': [],\n",
        "    'cum_EL_return': [],\n",
        "    'cum_ES_return': [],\n",
        "    'cum_ELS_return': [],\n",
        "    'cum_VL_return': [],\n",
        "    'cum_VS_return': [],\n",
        "    'cum_VLS_return': [],\n",
        "    'cum_market_return': []\n",
        "}\n",
        "\n",
        "# group by week_date and company to get the maximum probabilities for each company in a week\n",
        "grouped_max_neg = finbert_df.loc[finbert_df.groupby(['week_date', 'company'])['probability_neg'].idxmax()]\n",
        "grouped_max_pos = finbert_df.loc[finbert_df.groupby(['week_date', 'company'])['probability_pos'].idxmax()]\n",
        "\n",
        "# group by week_date\n",
        "grouped_neg = grouped_max_neg.groupby('week_date')\n",
        "grouped_pos = grouped_max_pos.groupby('week_date')\n",
        "\n",
        "# initialise cumulative log returns\n",
        "cum_EL_return = 0\n",
        "cum_ES_return = 0\n",
        "cum_ELS_return = 0\n",
        "cum_VL_return = 0\n",
        "cum_VS_return = 0\n",
        "cum_VLS_return = 0\n",
        "cum_market_return = 0\n",
        "\n",
        "# iterate over week\n",
        "for week in finbert_df['week_date'].unique():\n",
        "    group_neg = grouped_neg.get_group(week)\n",
        "    group_pos = grouped_pos.get_group(week)\n",
        "\n",
        "    # sort by probability for positive and negative predictions\n",
        "    top_positive = group_pos.sort_values(by='probability_pos', ascending=False).head(5)\n",
        "    top_negative = group_neg.sort_values(by='probability_neg', ascending=False).head(5)\n",
        "\n",
        "    # calculate log returns\n",
        "    top_positive['log_return'] = np.log1p(top_positive['weekly_ret'])\n",
        "    top_negative['log_return'] = np.log1p(top_negative['weekly_ret'])\n",
        "    finbert_df.loc[finbert_df['week_date'] == week, 'log_return'] = np.log1p(finbert_df.loc[finbert_df['week_date'] == week, 'weekly_ret'])\n",
        "\n",
        "    # Equal-Weighted Long Log Returns\n",
        "    equal_long_log_return = top_positive['log_return'].mean()\n",
        "\n",
        "    # Equal-Weighted Short Log Returns\n",
        "    equal_short_log_return = top_negative['log_return'].mean() * -1  # Negate for short\n",
        "\n",
        "    # Equal-Weighted Long-Short Log Returns\n",
        "    equal_long_short_log_return = equal_long_log_return + equal_short_log_return\n",
        "\n",
        "    # Value-Weighted Long Log Returns based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap'].sum()\n",
        "    value_long_log_return = (top_positive['log_return'] * top_positive['market_cap']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short Log Returns based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap'].sum()\n",
        "    value_short_log_return = (top_negative['log_return'] * top_negative['market_cap']).sum() / total_market_cap_negative * -1  # Negate for short\n",
        "\n",
        "    # Value-Weighted Long-Short Log Returns\n",
        "    value_long_short_log_return = value_long_log_return + value_short_log_return\n",
        "\n",
        "    # Market Log Return (equal-weighted average of all assets in the group)\n",
        "    market_log_return = finbert_df.loc[finbert_df['week_date'] == week, 'log_return'].mean()\n",
        "\n",
        "    # update cumulative log returns\n",
        "    cum_EL_return += equal_long_log_return\n",
        "    cum_ES_return += equal_short_log_return\n",
        "    cum_ELS_return += equal_long_short_log_return\n",
        "    cum_VL_return += value_long_log_return\n",
        "    cum_VS_return += value_short_log_return\n",
        "    cum_VLS_return += value_long_short_log_return\n",
        "    cum_market_return += market_log_return\n",
        "\n",
        "    # append results for this date\n",
        "    cumulative_log_returns_by_date['date'].append(week)\n",
        "    cumulative_log_returns_by_date['cum_EL_return'].append(cum_EL_return)\n",
        "    cumulative_log_returns_by_date['cum_ES_return'].append(cum_ES_return)\n",
        "    cumulative_log_returns_by_date['cum_ELS_return'].append(cum_ELS_return)\n",
        "    cumulative_log_returns_by_date['cum_VL_return'].append(cum_VL_return)\n",
        "    cumulative_log_returns_by_date['cum_VS_return'].append(cum_VS_return)\n",
        "    cumulative_log_returns_by_date['cum_VLS_return'].append(cum_VLS_return)\n",
        "    cumulative_log_returns_by_date['cum_market_return'].append(cum_market_return)\n",
        "\n",
        "# convert to dataframe\n",
        "cumulative_log_returns_finbert = pd.DataFrame(cumulative_log_returns_by_date)"
      ],
      "metadata": {
        "id": "xvpOp2P78mY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save results\n",
        "path_finbert = '/content/drive/MyDrive/PostGrad/5. Extended Research Projects/Results/Portfolio Analysis - Pre Trained/finbert_portfolio_pretrained.csv'\n",
        "\n",
        "# save to csv\n",
        "cumulative_log_returns_finbert.to_csv(path_finbert, index=False)"
      ],
      "metadata": {
        "id": "FAvvYl_V8mY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sharpe Ratio**"
      ],
      "metadata": {
        "id": "fUh2yXed8mY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate weekly log returns from cumulative log returns\n",
        "log_returns = cumulative_log_returns_finbert.set_index('date').diff().dropna()\n",
        "\n",
        "# define function to calculate Sharpe Ratio\n",
        "def calculate_sharpe_ratio(return_series, risk_free_rate=0):\n",
        "    mean_return = return_series.mean()\n",
        "    std_return = return_series.std()\n",
        "    excess_return = mean_return - risk_free_rate\n",
        "    sharpe_ratio = (excess_return / std_return) * np.sqrt(52)\n",
        "    return mean_return, std_return, sharpe_ratio\n",
        "\n",
        "sharpe_ratios = []\n",
        "\n",
        "for column in log_returns.columns:\n",
        "    mean_return, std_return, sharpe_ratio = calculate_sharpe_ratio(log_returns[column])\n",
        "    sharpe_ratios.append({\n",
        "        'Portfolio': column,\n",
        "        'Mean Return': mean_return,\n",
        "        'Standard Deviation': std_return,\n",
        "        'Sharpe Ratio': sharpe_ratio\n",
        "    })\n",
        "\n",
        "# convert to dataframe\n",
        "sharpe_ratios_df = pd.DataFrame(sharpe_ratios)"
      ],
      "metadata": {
        "id": "QDOfTj8T8mY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharpe_ratios_df"
      ],
      "metadata": {
        "id": "r0fwkPrV8mY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect run time\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "O8UxdIbh8mY3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}